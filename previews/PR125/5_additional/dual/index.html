<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Automatic Differentiation · A fresh approach to scientific computing</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://formal-methods-mpi.github.io/Workshop.jl/5_additional/dual/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="A fresh approach to scientific computing logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="A fresh approach to scientific computing logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">A fresh approach to scientific computing</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../0_preparation/preparation/">Chapter 0: Preparation</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Chapter 1: Workflow</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../1_workflow/usage/">Working with Julia in VSCode</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Chapter 2: Syntax</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../2_syntax/foundations/">Theory</a></li><li><a class="tocitem" href="../../2_syntax/exercise/">Exercise</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">Chapter 3: Types</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../3_types/types/">Theory</a></li><li><a class="tocitem" href="../../3_types/exercise/">Exercise</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">Chapter 4: Multiple Dispatch</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../4_dispatch/dispatch/">Theory</a></li><li><a class="tocitem" href="../../4_dispatch/exercise/">Exercise</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox" checked/><label class="tocitem" for="menuitem-7"><span class="docs-label">Additional Materials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../intro/">Introduction</a></li><li><a class="tocitem" href="../extensive-foundations/">Foundations Revisited</a></li><li class="is-active"><a class="tocitem" href>Automatic Differentiation</a></li><li><a class="tocitem" href="../links/">Useful Links</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Additional Materials</a></li><li class="is-active"><a href>Automatic Differentiation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Automatic Differentiation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/formal-methods-mpi/Workshop.jl/blob/main/docs/src/5_additional/dual.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Automatic-Differentiation"><a class="docs-heading-anchor" href="#Automatic-Differentiation">Automatic Differentiation</a><a id="Automatic-Differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Differentiation" title="Permalink"></a></h1><p>Obtaining function derivatives is central in many areas of scientific computing. For example, in numerical optimization, we often need derivatives to find the minimum of a function (maximum likelihood estimation, neural networks, ...). However, deriving derivatives by hand can be quite tedious, especially if you work with frequently changing functions. As numerical differentiation is computationally quite expensive, automatic differentiation libraries have been developed. For example, most Deep Learning libraries employ a form of automatic differentiation called &quot;backpropagation&quot;. In this example, we will implement one form of automatic differentiation, called &quot;Forward mode AD&quot;.<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup></p><p>To do so, we heavily rely on the chain rule:  suppose we have functions <span>$f: \mathbb{R} \to \mathbb{R}, \; g: \mathbb{R} \to \mathbb{R}$</span>. If we compose these functions, we can derive their derivative as </p><p class="math-container">\[(f(g(x)))&#39; = f&#39;(g(x)) \cdot g&#39;(x)\]</p><p>With a little bit abuse of notation, instead of defining two functions <span>$f$</span> and <span>$g$</span> we will write in the following <span>$x: \mathbb{R} \to \mathbb{R}$</span> as a function itself, so the chain rule becomes:</p><p class="math-container">\[(f(x))&#39; = f&#39;(x) \cdot x&#39;\]</p><p>Using the chain rule together with all the other basic differentiation rules you hopefully remember from high school gives:</p><p class="math-container">\[\begin{aligned}
(x + y)&#39; &amp;= x&#39; + y&#39; \\
(x - y)&#39; &amp;= x&#39; - y&#39; \\
(x \cdot y)&#39; &amp;= x&#39;\cdot y + x \cdot y&#39; \\
\left(\frac{1}{x}\right)&#39; &amp;= \frac{-x&#39;}{x^2} \\
\left(\frac{x}{y}\right)&#39; &amp;= \frac{x&#39; \cdot y - x \cdot y&#39;}{y^2} \\
(\log(x))&#39; &amp;= \frac{1}{x} \cdot x&#39; \\
(\sin(x))&#39; &amp;= \cos(x) \cdot x&#39; \\
(\cos(x))&#39; &amp;= -\sin(x) \cdot x&#39; \\
\left(x^k\right)&#39; &amp;= k \cdot x^{k-1} \cdot x&#39;
\end{aligned}\]</p><p>As most functions are compositions of basic functions (of course our list above is not exhaustive, but already sufficient for a lot of things), we can use the chain rule and the differentiation rules for those basic building blocks to &quot;propagate&quot; the derivative through more complex functions (just like we propagated the measurement error in the example in Chapter 3).</p><div class="admonition is-compat"><header class="admonition-header">Exercise</header><div class="admonition-body"><p>Define a struct <code>Dual</code> that stores a function value as well as it&#39;s derivative as a subtype of <code>Number</code>.</p></div></div><details>
<summary>show solution</summary>
<br><div class="admonition is-success"><header class="admonition-header">Solution</header><div class="admonition-body"><pre><code class="language-julia hljs">struct Dual &lt;: Number
    x
    ∂x
end</code></pre></div></div></details><p><br/></p><div class="admonition is-compat"><header class="admonition-header">Exercise</header><div class="admonition-body"><p>Define methods for some of the basic functions described above: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>inv</code>, <code>log</code>, <code>sin</code>, <code>cos</code>, <code>^</code>. Remember that you have to import them from <code>Base</code>. Don&#39;t do all of them (it&#39;s tedious), just a few to get the idea.</p></div></div><details>
<summary>Hint</summary>
<br><div class="admonition is-compat"><header class="admonition-header">Hint</header><div class="admonition-body"><p>We could define the multiplication of two dual numbers as:</p><pre><code class="language-julia hljs">import Base: *
*(a::Dual, b::Dual) = Dual(a.x*b.x, a.∂x*b.x + a.x*b.∂x)</code></pre></div></div></details><p><br/></p><details>
<summary>show solution</summary>
<br><div class="admonition is-success"><header class="admonition-header">Solution</header><div class="admonition-body"><pre><code class="language-julia hljs">import Base: +, -, *, /, inv, log, sin, cos, ^

+(a::Dual, b::Dual) = Dual(a.x + b.x, a.∂x + b.∂x)
-(a::Dual, b::Dual) = Dual(a.x - b.x, a.∂x - b.∂x)
*(a::Dual, b::Dual) = Dual(a.x * b.x, a.∂x*b.x + a.x*b.∂x)
^(a::Dual, k::Int) = Dual(a.x^k, k*a.x^(k-1)*a.∂x)
/(a::Dual, b::Dual) = Dual(a.x/b.x, (a.∂x*b.x - a.x*b.∂x)/b.x^2)
inv(a::Dual) = Dual(inv(a.x), -a.∂x/(a.x^2))
log(a::Dual) = Dual(log(a.x), -a.∂x/(a.x^2))
sin(a::Dual) = Dual(sin(a.x), cos(a.x)*a.∂x)
cos(a::Dual) = Dual(cos(a.x), -sin(a.x)*a.∂x)</code></pre></div></div></details><p><br/>Of course we also want to be able to have constants in our functions. To make our life a little bit easier, we can treat constant values (aka real numbers) as dual numbers with a derivative of 0. For this purpose, we define how to convert a real number into a dual number, and a &quot;promotion rule&quot;:</p><pre><code class="language-julia hljs">import Base: convert, promote_rule

convert(::Type{Dual}, x::Real) = Dual(x, zero(x))
promote_rule(::Type{Dual}, ::Type{&lt;:Real}) = Dual</code></pre><p>We did not cover conversion and promotion in this workshop, so you won&#39;t understand this in detail, but the short explanation is this: We write a method for the <code>convert</code> function to tell Julia how to &quot;convert&quot; an object of type <code>Real</code> to an object of type <code>Dual</code>, so we can now do something like this:</p><pre><code class="language-julia hljs">a = 3.23
convert(Dual, a)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Main.Dual(3.23, 0.0)</code></pre><p>that is, a Real number is converted to a dual with a derivative of zero. The promotion rule defines a greater type of two types so that all objects of these types can be converted to it. We now can do something like:</p><pre><code class="language-julia hljs">a = 3.23
b = Dual(3.2, 3.47)
promote(a, b)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(Main.Dual(3.23, 0.0), Main.Dual(3.2, 3.47))</code></pre><p>So given a pair of a <code>Dual</code> and a <code>Real</code> number, promote both numbers to <code>Dual</code>s. The beauty of this is that we now can add, multiply, divide, etc. reals and duals without defining any further methods:</p><pre><code class="language-julia hljs">a = 3.23
b = Dual(3.2, 3.47)
a*b</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Main.Dual(10.336, 11.2081)</code></pre><pre><code class="language-julia hljs">a+b</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Main.Dual(6.43, 3.47)</code></pre><pre><code class="language-julia hljs">a/b</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Main.Dual(1.009375, -1.0945410156249997)</code></pre><p>This is because Julia will now &quot;check&quot; the defined promotion rules for us, and promote the real number to a dual number, and then use the methods we defined before. If you are interested in details, check the documentation on this topic, but for now it suffices to see that we avoided defining a bunch of methods with this neat trick.</p><p>Now we have implemented a library for automatic differentiation! Let&#39;s come up with some random function to take the derivative of:</p><pre><code class="language-julia hljs">f(x) = (x^2*sin(x))/cos(x)^2</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">f (generic function with 1 method)</code></pre><p>To compute the derivative at point <code>x</code>, simply call the function with <code>Dual(x, 1.0)</code> (as <code>1.0</code> is the derivative of the identity function <code>x</code>):</p><pre><code class="language-julia hljs">f(Dual(5.0, 1.0))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Main.Dual(-297.9349362980947, 1983.306073707824)</code></pre><p>This gives the function value and the derivative of <code>f</code> at point <code>5</code>. Another function and it&#39;s derivative:</p><pre><code class="language-julia hljs">g(x) = log(x)/sin(x^5)
g(Dual(π, 1.0))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Main.Dual(-1.193056943017974, 170.2864202728395)</code></pre><p>We can check that we did not messed up by either deriving the derivative by hand or using a numerical approximation:</p><pre><code class="language-julia hljs">using FiniteDiff
FiniteDiff.finite_difference_derivative(g, Float64(π))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">170.29979687131868</code></pre><p>We can also take the partial derivative of functions <span>$h: \mathbb{R^n} \to \mathbb{R}$</span>:</p><pre><code class="language-julia hljs">h(x) = sin(x[1]*x[2]^2-x[3])/log(x[1])
h([4.3, Dual(6.45, 1.0), 3.42])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Main.Dual(-0.303403203747067, 34.102502278364675)</code></pre><p>gives the partial derivative w.r.t. <span>$x_2$</span> at the point <span>$(4.3, 6.45, 3.42)$</span>:</p><pre><code class="language-julia hljs">FiniteDiff.finite_difference_gradient(h, [4.3, 6.45, 3.42])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Float64}:
 25.625245570490197
 34.10247571021924
 -0.614791820655715</code></pre><p>To sum up, let&#39;s get back to our example from the beginning: linear regression. Recall our setup:</p><pre><code class="language-julia hljs">using Random
Random.seed!(1243)

x = 10 .+ 3*randn(20)
β = 300
α = 1000
y = α .+ β*x + 500*randn(20)

function predict(x, α, β)
    y = α .+ β*x
    return y
end

function squared_error(y, ŷ)
    return sum((y - ŷ).^2)
end</code></pre><div class="admonition is-compat"><header class="admonition-header">Exercise</header><div class="admonition-body"><p>Compute the derivative of the squared error w.r.t. <code>β</code> at the point <code>β = 200, α = 1000</code>.</p></div></div><details>
<summary>Hint</summary>
<br><div class="admonition is-compat"><header class="admonition-header">Hint</header><div class="admonition-body"><p>Recall you can compute the squared error as</p><pre><code class="language-julia hljs">squared_error(y, predict(x, α, β))</code></pre></div></div></details><p><br/></p><details>
<summary>show solution</summary>
<br><div class="admonition is-success"><header class="admonition-header">Solution</header><div class="admonition-body"><pre><code class="language-julia hljs">squared_error(y, predict(x, 1000, Dual(200.0, 1.0)))</code></pre></div></div><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Main.Dual(2.511919957961294e7, -431198.66352834756)</code></pre></details><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>This example is inspired by https://www.wias-berlin.de/people/fuhrmann/AdSciComp-WS2223/week3/</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../extensive-foundations/">« Foundations Revisited</a><a class="docs-footer-nextpage" href="../links/">Useful Links »</a><div class="flexbox-break"></div><p class="footer-message">Got a question? Something confuses you? <a href="https://github.com/formal-methods-mpi/Workshop.jl/issues/new">Don&#39;t hesitate to open an issue to ask for help!</a></p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Sunday 10 September 2023 14:52">Sunday 10 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
